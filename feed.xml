<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neslihan Bayramoglu</title>
    <description></description>
    <link>http://www.ee.oulu.fi/~nyalcinb/</link>
    <atom:link href="http://www.ee.oulu.fi/~nyalcinb/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 25 Mar 2020 10:58:02 +0200</pubDate>
    <lastBuildDate>Wed, 25 Mar 2020 10:58:02 +0200</lastBuildDate>
    <generator>Jekyll v3.8.4</generator>
    
      <item>
        <title>Adaptive Segmentation of Knee Radiographs for Selecting the Optimal ROI in Texture Analysis</title>
        <description>&lt;h6&gt;Osteoarthritis and Cartilage (OCJ), 2020.&lt;/h6&gt;

&lt;hr&gt;

&lt;p&gt;&lt;a href=&quot;{https://arxiv.org/abs/1908.07736&quot; onclick=&quot;ga('send', 'event', 'click', 'click to ocj2022');&quot;&gt; &lt;font color=&quot;FF00CC&quot;&gt; Paper Link &lt;/font&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt;&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;Objective
The purposes of this study were to investigate: 1) the effect of placement of region-of-interest (ROI) for texture analysis of subchondral bone in knee radiographs, and 2) the ability of several texture descriptors to distinguish between the knees with and without radiographic osteoarthritis (OA).&lt;/p&gt;

&lt;h3&gt;Design&lt;/h3&gt;

&lt;p&gt;Bilateral posterior-anterior knee radiographs were analyzed from the baseline of Osteoarthritis Initiative (OAI) (9012 knee radiographs) and Multicenter Osteoarthritis Study (MOST) (3644 knee radiographs) datasets. A fully automatic method to locate the most informative region from subchondral bone using adaptive segmentation was developed. Subsequently, we built logistic regression models to identify and compare the performances of several texture descriptors and each ROI placement method using 5-fold cross validation. Importantly, we also investigated the generalizability of our approach by training the models on OAI and testing them on MOST dataset. We used area under the receiver operating characteristic (ROC) curve (AUC) and average precision (AP) obtained from the precision-recall (PR) curve to compare the results.&lt;/p&gt;

&lt;h3&gt;Results&lt;/h3&gt;

&lt;p&gt;We found that the adaptive ROI improves the classification performance (OA vs. non-OA) over the commonly-used standard ROI (up to 9% percent increase in AUC). We also observed that, from all texture parameters, Local Binary Pattern (LBP) yielded the best performance in all settings with the best AUC of 0.840 [0.825, 0.852] and associated AP of 0.804 [0.786, 0.820].&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Compared to the current state-of-the-art approaches, our results suggest that the proposed adaptive ROI approach in texture analysis of subchondral bone can increase the diagnostic performance for detecting the presence of radiographic OA.&lt;/p&gt;

&lt;h3&gt;Keywords&lt;/h3&gt;

&lt;p&gt;Osteoarthritisbone texture analysisadaptive region of interestkneeradiograph&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/ocj2020/ocj2020_1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; width=300 src=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/ocj2020/ocj2020_1.png&quot;/&gt;
            &lt;/a&gt;&lt;/p&gt;

&lt;hr&gt;
</description>
        <pubDate>Mon, 23 Mar 2020 00:00:00 +0200</pubDate>
        <link>http://www.ee.oulu.fi/~nyalcinb/papers/oac2020</link>
        <guid isPermaLink="true">http://www.ee.oulu.fi/~nyalcinb/papers/oac2020</guid>
        
        <category>OCJ-Adaptive</category>
        
        
        <category>pub</category>
        
      </item>
    
      <item>
        <title>Towards Virtual H&amp;E Staining of Hyperspectral Lung Histology Images Using Conditional Generative Adversarial Networks</title>
        <description>&lt;h6&gt;International Conference on Computer Vision (ICCV), 2017.&lt;/h6&gt;

&lt;h6&gt;Workshop: BIC:Bioimage Computing.&lt;/h6&gt;

&lt;hr&gt;

&lt;!--&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/iccv2017/paper.pdf&quot; target=&quot;_blank&quot;
onClick=&quot;ga('send', 'event', 'click to iccv2017_paper', 'click to iccv2017_paper');&quot;&gt;Download paper pdf&lt;/a&gt;

&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/iccv2017/presentation.pdf&quot; target=&quot;_blank&quot;
onClick=&quot;ga('send', 'event', 'click to iccv2017_presentation', 'click to iccv2017_presentation');&quot;&gt;Presentation&lt;/a&gt;

&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/iccv2017/Poster.pdf&quot; target=&quot;_blank&quot;
onClick=&quot;ga('send', 'event', 'click to iccv2017_Poster', 'click to iccv2017_Poster');&quot;&gt;Poster (26.2MB)&lt;/a&gt;
--&gt;

&lt;p&gt;Download  &lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/papers/hsi_toolbox&quot;&gt;matlab toobox&lt;/a&gt; to manipulate hyperspectral data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/data/bayramoglu_data_iccv_2017.tar.gz&quot; onclick=&quot;ga('send', 'event', 'click', 'click to iccv2017data');&quot;&gt; &lt;font color=&quot;FF00CC&quot;&gt; Download data &lt;/font&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt;&lt;/p&gt;

&lt;!--&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/papers/eccv2016/index.html&quot; target=&quot;_blank&quot;--&gt;

&lt;!--onClick=&quot;ga('send', 'event', 'click toeccv2016_paper', 'click to eccv2016_paper');&quot;&gt;Download paper pdf (will be available soon)&lt;/a&gt; --&gt;

&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The microscopic image of a specimen in the absence of staining appears colorless and textureless. Therefore, microscopic inspection of tissue requires chemical staining to create contrast. Hematoxylin and eosin (H&amp;amp;E) is the most widely used chemical staining technique in histopathology. However, such staining creates obstacles for automated image analysis systems. Due to different chemical formulations, different scanners, section thickness, and lab protocols, similar tissues can greatly differ in appearance. This huge variability is one of the main challenges in designing robust and resilient automated image analysis systems. Moreover, staining process is time consuming and its chemical effects deform structures of specimens.&lt;/p&gt;

&lt;p&gt;In this work, we develop a method to virtually stain unstained specimens. Our method utilizes dimension reduction and conditional adversarial generative networks (cGANs) which build highly non-linear mappings between input and output images. Conditional GANs ability to handle very complex functions and high dimensional data enables transforming unstained hyperspectral tissue image to their H&amp;amp;E equivalent which comprises highly diversified appearance. In the long term, such virtual digital H&amp;amp;E staining could automate some of the tasks in the diagnostic pathology workflow which could be used to speed up the sample processing time, reduce costs, prevent adverse effects of chemical stains on tissue specimens, reduce observer variability, and increase objectivity in disease diagnosis.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/iccv2017/iccv2017_1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; width=300 src=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/iccv2017/iccv2017_1.png&quot;/&gt;
            &lt;/a&gt;&lt;/p&gt;

&lt;hr&gt;
</description>
        <pubDate>Wed, 23 Aug 2017 00:00:00 +0300</pubDate>
        <link>http://www.ee.oulu.fi/~nyalcinb/papers/iccv2017</link>
        <guid isPermaLink="true">http://www.ee.oulu.fi/~nyalcinb/papers/iccv2017</guid>
        
        <category>ICCV-</category>
        
        <category>BIC</category>
        
        
        <category>pub</category>
        
      </item>
    
      <item>
        <title>A MATLAB Toolbox for Hyperspectral Data (for ENVI Format)</title>
        <description>&lt;h6&gt;A MATLAB Toolbox for Hyperspectral Data (for ENVI Format)&lt;/h6&gt;

&lt;hr&gt;

&lt;p&gt;Download &lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/hsi_toolbox/hsi_toolbox.tar.gz&quot;
onClick=&quot;ga('send', 'event', 'click to hsi_toolbox', 'click to hsi_toolbox');&quot;&gt; toolbox. &lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Toolbox comes with a user guide that explains how to manipulate hyperspectral data in ENVI format within the MATLAB environment. The toolbox includes several functions to read data from .hdr file into a MATLAB matrix (3 dimensional), visualize wavelengths separately with different color mappings, functions for normalization of hyperspectral data with reference, writing hyperspectral cube as tiff file, principal component analysis, visualization of point spectral properties, and hypercube alignment (registration).&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;Examplar function from the toolbox: plotpointspectra&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Spectrum of the clicked points are written to a text file. Two figures will be displayed. Clicked points are marked on the image and its spectra is plotted on the second figure with same color.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/hsi_toolbox/toolbox_1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; width=400 src=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/hsi_toolbox/toolbox_1.png&quot;/&gt;
            &lt;/a&gt;
 &lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/hsi_toolbox/toolbox_2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; width=400 src=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/hsi_toolbox/toolbox_2.png&quot;/&gt;
            &lt;/a&gt;&lt;/p&gt;

&lt;hr&gt;
</description>
        <pubDate>Wed, 23 Aug 2017 00:00:00 +0300</pubDate>
        <link>http://www.ee.oulu.fi/~nyalcinb/papers/hsi_toolbox</link>
        <guid isPermaLink="true">http://www.ee.oulu.fi/~nyalcinb/papers/hsi_toolbox</guid>
        
        <category>Toolbox</category>
        
        
        <category>pub</category>
        
      </item>
    
      <item>
        <title>Transfer Learning for Cell Nuclei Classification in Histopathology Images</title>
        <description>&lt;h6&gt;The 14th European Conference on Computer Vision, 2016.&lt;/h6&gt;

&lt;h6&gt;Workshop: TASK-CV:Transferring and Adapting Source Knowledge in Computer Vision.&lt;/h6&gt;

&lt;hr&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/eccv2016/paper.pdf&quot; target=&quot;_blank&quot;
onClick=&quot;ga('send', 'event', 'click to eccv2016_paper', 'click to eccv2016_paper');&quot;&gt;Download paper pdf&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/eccv2016/presentation.pdf&quot; target=&quot;_blank&quot;
onClick=&quot;ga('send', 'event', 'click to eccv2016_presentation', 'click to eccv2016_presentation');&quot;&gt;Presentation&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/eccv2016/Poster.pdf&quot; target=&quot;_blank&quot;
onClick=&quot;ga('send', 'event', 'click to eccv2016_Poster', 'click to eccv2016_Poster');&quot;&gt;Poster (26.2MB)&lt;/a&gt; &lt;/p&gt;

&lt;!--&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/papers/eccv2016/index.html&quot; target=&quot;_blank&quot;--&gt;

&lt;!--onClick=&quot;ga('send', 'event', 'click toeccv2016_paper', 'click to eccv2016_paper');&quot;&gt;Download paper pdf (will be available soon)&lt;/a&gt; --&gt;

&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In histopathological image assessment, there is a high demand to obtain fast and precise quantification automatically. Such automation could be beneficial to find clinical assessment clues to produce correct diagnoses, to reduce observer variability, and to increase objectivity. Due to its success in other areas, deep learning could be the key method to obtain clinical acceptance. However, the major bottleneck is how to train a deep CNN model with a limited amount of training data. There is one important question of critical importance: Could it be possible to use transfer learning and fine-tuning  in biomedical image analysis to reduce the effort of manual data labeling and still obtain a full deep representation for the target task? In this study, we address this question quantitatively by comparing the performances of transfer learning and learning from scratch for cell nuclei classification. We evaluate four different CNN architectures trained on natural images and facial images.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/eccv2016/eccv2016_1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; width=600 src=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/eccv2016/eccv2016_1.png&quot;/&gt;
            &lt;/a&gt;&lt;/p&gt;

&lt;hr&gt;
</description>
        <pubDate>Mon, 22 Aug 2016 00:00:00 +0300</pubDate>
        <link>http://www.ee.oulu.fi/~nyalcinb/papers/eccv2016</link>
        <guid isPermaLink="true">http://www.ee.oulu.fi/~nyalcinb/papers/eccv2016</guid>
        
        <category>ECCV-WS</category>
        
        
        <category>pub</category>
        
      </item>
    
      <item>
        <title>Deep Learning for Magnification Independent Breast Cancer Histopathology Image Classification</title>
        <description>&lt;h6&gt;23rd International Conference on Pattern Recognition, 2016.&lt;/h6&gt;

&lt;hr&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/icpr2016/paper.pdf&quot; target=&quot;_blank&quot;
onClick=&quot;ga('send', 'event', 'click to icpr2016_paper', 'click to icpr2016_paper');&quot;&gt;Download paper pdf&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/icpr2016/presentation.pdf&quot; target=&quot;_blank&quot;
onClick=&quot;ga('send', 'event', 'click to icpr2016_presentation', 'click to icpr2016_presentation');&quot;&gt;Presentation pdf&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/presentations.html#icpr2016_presentation&quot; target=&quot;_blank&quot;
onClick=&quot;ga('send', 'event', 'click to icpr2016_presentation', 'click to icpr2016_presentation');&quot;&gt; View Presentation&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;
Microscopic analysis of breast tissues is necessary for a definitive diagnosis of breast cancer which is the most
common cancer among women. Pathology examination requires time consuming scanning through tissue images under different magnification levels to find clinical assessment clues to produce correct diagnoses. Advances in digital imaging techniques offers assessment of pathology images using computer vision and
machine learning methods which could automate some of the tasks in the diagnostic pathology workflow. Such automation could be beneficial to obtain fast and precise quantification, reduce observer variability, and increase objectivity. In this work, we propose to classify breast cancer histopathology images independent of their magnifications using convolutional neural networks (CNNs). We propose two different architectures; single task CNN is used to predict malignancy and multi-task CNN is used to predict both malignancy and image
magnification level simultaneously. Evaluations and comparisons with previous results are carried out on BreaKHis dataset. Experimental results show that our magnification independent CNN approach improved the performance of magnification specific model. Our results in this limited set of training data are comparable with previous state-of-the-art results obtained by hand-crafted features. However, unlike previous methods, our approach has potential to directly benefit from additional training data, and such additional data could be captured with same or
different magnification levels than previous data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/icpr2016/icpr2016_1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; width=600 src=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/icpr2016/icpr2016_1.png&quot;/&gt;
            &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/icpr2016/icpr2016_2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; width=300 src=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/icpr2016/icpr2016_2.png&quot;/&gt;
            &lt;/a&gt;&lt;/p&gt;

&lt;hr&gt;
</description>
        <pubDate>Wed, 13 Jul 2016 00:00:00 +0300</pubDate>
        <link>http://www.ee.oulu.fi/~nyalcinb/papers/icpr2016</link>
        <guid isPermaLink="true">http://www.ee.oulu.fi/~nyalcinb/papers/icpr2016</guid>
        
        <category>ICPR</category>
        
        
        <category>pub</category>
        
      </item>
    
      <item>
        <title>Oulu</title>
        <description>&lt;p&gt;Oulu&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/blog/ranta.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot;  src=&quot;http://www.ee.oulu.fi/~nyalcinb/images/blog/ranta.png&quot; /&gt;
            &lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 11 Feb 2016 00:00:00 +0200</pubDate>
        <link>http://www.ee.oulu.fi/~nyalcinb/blog/2016/02/11/blog.html</link>
        <guid isPermaLink="true">http://www.ee.oulu.fi/~nyalcinb/blog/2016/02/11/blog.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>OpenCV version on Ubuntu</title>
        <description>&lt;p&gt;In order to see the OpenCV version on Ubuntu 14.04 you can use this:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-&quot; data-lang=&quot;&quot;&gt;```
pkg-config --modversion opencv
```
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
        <pubDate>Fri, 08 Jan 2016 00:00:00 +0200</pubDate>
        <link>http://www.ee.oulu.fi/~nyalcinb/blog/2016/01/08/blog.html</link>
        <guid isPermaLink="true">http://www.ee.oulu.fi/~nyalcinb/blog/2016/01/08/blog.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Download multiples files from a website at once</title>
        <description>&lt;p&gt;To download multiple files in a directory from a website use &lt;code&gt;wget&lt;/code&gt; as follows: &lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-&quot; data-lang=&quot;&quot;&gt;```
wget -r -np http://base_adress.com/folder_name
```
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
        <pubDate>Tue, 01 Dec 2015 00:00:00 +0200</pubDate>
        <link>http://www.ee.oulu.fi/~nyalcinb/blog/2015/12/01/blog.html</link>
        <guid isPermaLink="true">http://www.ee.oulu.fi/~nyalcinb/blog/2015/12/01/blog.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Beamer tips</title>
        <description>&lt;ol&gt;
&lt;li&gt;&lt;p&gt;-If you get &amp;quot;xcolor package clash&amp;quot; in your beamer presentations use the following:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-&quot; data-lang=&quot;&quot;&gt;&lt;span class=&quot;k&quot;&gt;\documentclass&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;[xcolor={dvipsnames}]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;beamer&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;-To remove navigation icons from slides use:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-&quot; data-lang=&quot;&quot;&gt;\setbeamertemplate{navigation symbols}{}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 12 Nov 2015 00:00:00 +0200</pubDate>
        <link>http://www.ee.oulu.fi/~nyalcinb/blog/2015/11/12/blog.html</link>
        <guid isPermaLink="true">http://www.ee.oulu.fi/~nyalcinb/blog/2015/11/12/blog.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Human Epithelial Type 2 Cell Classification with Convolutional Neural Networks</title>
        <description>&lt;h6&gt;15th IEEE International Conference on Bioinformatics &amp;amp; Bioengineering (BIBE), 2015.&lt;/h6&gt;

&lt;hr&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/bibe2015/paper.pdf&quot; target=&quot;_blank&quot;
onClick=&quot;ga('send', 'event', 'click to bibe2015_paper', 'click to bibe2015_paper');&quot;&gt;Download paper pdf&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Presentation &lt;a href=&quot;http://www.ee.oulu.fi/~nyalcinb/presentations.html#bibe2015_presentation&quot; target=&quot;_blank&quot;
onClick=&quot;ga('send', 'event', 'click to bibe2015_presentation', 'click to bibe2015_presentation');&quot;&gt; pdf.&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;
Automated cell classification in Indirect Immunofluorescence (IIF) images has potential to be an important tool in clinical practice and research. This paper presents a framework for classification of Human Epithelial Type 2 cell IIF images using convolutional neural networks (CNNs). Previuos state-of-the-art methods show classification accuracy of 75.6% on a benchmark dataset. We conduct an exploration of different strategies for enhancing, augmenting and processing training data in a CNN framework for image classification. Our proposed strategy for training data and pre-training and fine tuning the CNN network led to a significant increase in the performance over other approaches that have been used until now. Specifically, our method achieves a 80.25% classification accuracy.&lt;/p&gt;

&lt;h6&gt;Download CNN Caffe Model and Source:&lt;/h6&gt;

&lt;p&gt;The trained caffe model used in the paper can be downloaded here: 
&lt;a href= &quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/bibe2015/t21_iter_110000.caffemodel&quot; onClick=&quot;ga('send', 'event', 'click to bibeModel', 'click to bibeModel');&quot;&gt;  Download &amp;quot;Caffe Model&amp;quot; &lt;/a&gt;&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;Caffe proto file for training/testing:
&lt;a href= &quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/bibe2015/train_test.prototxt&quot; onClick=&quot;ga('send', 'event', 'click to bibeproto', 'click to bibeproto');&quot;&gt;  Download caffe proto file&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;Caffe solver file:
&lt;a href= &quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/bibe2015/solver.prototxt&quot; onClick=&quot;ga('send', 'event', 'click to bibeSolver', 'click to bibeSolver');&quot;&gt;  Download solver file&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;Source files for data processing and augmentation (depends on the OpenCV library):
&lt;a href= &quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/bibe2015/source.tar.gz&quot; onClick=&quot;ga('send', 'event', 'click to bibeSource', 'click to bibeSource');&quot;&gt;  Download source files&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;
The network is shown below: 
&lt;/p&gt;

&lt;p&gt;&lt;embed src=&quot;http://www.ee.oulu.fi/~nyalcinb/images/pub/bibe2015/net.pdf&quot; width=&quot;718&quot; height=&quot;200&quot; alt=&quot;pdf&quot; pluginspage=&quot;http://www.adobe.com/products/acrobat/readstep2.html&quot;&gt;&lt;/p&gt;

&lt;hr&gt;
</description>
        <pubDate>Fri, 30 Oct 2015 09:33:00 +0200</pubDate>
        <link>http://www.ee.oulu.fi/~nyalcinb/papers/bibe2015</link>
        <guid isPermaLink="true">http://www.ee.oulu.fi/~nyalcinb/papers/bibe2015</guid>
        
        <category>BIBE</category>
        
        
        <category>pub</category>
        
      </item>
    
  </channel>
</rss>
